{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Saturdays, Boise - Week 6 (June 30, 2018)\n",
    "#### AI Developers Boise\n",
    "<img src=\"ai6-banner.png\" height=400 width=650/>\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "<img src=\"metageek-logo.svg\" height=200 width=400/>\n",
    "- **For supporting the workshop and sponsoring for this week's refreshment for participants**\n",
    "\n",
    "## Notes from Lecture 2, CS231n\n",
    "\n",
    "Prepared by:\n",
    "- Ashish Sharma <accssharma@gmail.com>\n",
    "\n",
    "## Image Classification\n",
    "\n",
    "- Core task in computer vision\n",
    "- Easy for humans - experience, pre-existing knowledge about objects, extremely good ability to extract and understand abstract ideas via vision\n",
    "- For computers, image classification is very challenging\n",
    "    - Image = just a grid of pixel values\n",
    "    - View point\n",
    "    - Illumination\n",
    "    - Deformation\n",
    "    - Occlusion\n",
    "    - Background color\n",
    "    - Interclass (also intra-class) variations\n",
    "    \n",
    "    \n",
    "- No obvious way to hard-code algorithm to recognize objects, explicitly programming is challenging and will result in limited capability\n",
    "\n",
    "- Solution?\n",
    "    - Data driven approach\n",
    "        - Collect dataset of images and labels\n",
    "        - Use ML to train a classifier\n",
    "        - Evaluate the classifier on new images\n",
    "        \n",
    "#### Simplest Classifier: Nearest Neighbor\n",
    "- Memorize all data and labels\n",
    "- predict the label based on the similarity metric\n",
    "\n",
    "\n",
    "#### Distance metrics\n",
    "- L1 distance (sum of pixel-wise absolute value differences)\n",
    "- L2 (Euclidean) distance (square-root of pixel-wise squared difference)\n",
    "- Cosine similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classifier: Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Memorize training data\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tr_size = self.X.shape[0]\n",
    "        self.dim = self.X.shape[1]\n",
    "    \n",
    "    def __similarity(self, frm):\n",
    "        dist = np.sum(np.abs(self.X - frm), axis=1)\n",
    "        return dist\n",
    "        \n",
    "    def predict(self, X_pred):\n",
    "        \"\"\"\"\"\"\n",
    "        assert type(X_pred) in [list, np.ndarray]\n",
    "        if type(X_pred) == list:\n",
    "            X_pred = np.array(X_pred)\n",
    "        assert X_pred.shape[0] > 0\n",
    "        assert type(X_pred[0])  in [list, np.ndarray]\n",
    "        \n",
    "        num_test = X_pred.shape[0]\n",
    "        y_pred = np.zeros(num_test, dtype=self.y.dtype)\n",
    "        \n",
    "        for ind, x_pred in enumerate(X_pred):\n",
    "            dist = self.__similarity(x_pred)\n",
    "            print(dist)\n",
    "            assert len(dist) == self.tr_size\n",
    "            min_index = np.argmin(dist)\n",
    "            y_pred[ind] = self.y[min_index]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9], [10, 11, 12]])\n",
    "y = np.array([1,1,0,0])\n",
    "X_pred = [[1,7,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  8  7 16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NearestNeighbor()\n",
    "nn.train(X, y)\n",
    "nn.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime\n",
    "Train: O(1)\n",
    "Test: O(N)\n",
    "\n",
    "- Exactly opposite of generally what we want. Taking some time for training is ok, but we would like to predict much faster.\n",
    "\n",
    "\n",
    "### k=1\n",
    "- island problem\n",
    "- not immune to noisy, spurious data points\n",
    "\n",
    "### k >1 => k-NN\n",
    "- instead of assigning the class based on a single nearest neighbor, we take k closest neaighbors accourding to the distance metric, and choose a label based on the vote from the nearest neighbors or vote weighted on distance, etc.\n",
    "\n",
    "## How to choose k?  How do you choose distance metric?\n",
    "    - try and error - educated guesses\n",
    "    - problem dependent\n",
    "    \n",
    "## Setting hyperparameters\n",
    "- Choose hyperparameters based on data? Terrible idea! NN with k=1 works best with the training data. Prone to overfitting\n",
    "\n",
    "## Train/Test \n",
    "- Terrible idea -> no idea how algorithm will perform on new data\n",
    "\n",
    "## Train/Val/Test\n",
    "- Train on train\n",
    "- choose your model on unseen validation set and choose hyperparameters based on this\n",
    "- Evaluate your model on test data\n",
    "- Train/Val set should be from same probability distribution if possible\n",
    "\n",
    "## Cross-validation \n",
    "- used in small scale data and traditional ML\n",
    "- not so much practical in large Neural Networks, more advanced regularization techniques are used to control overfitting\n",
    "\n",
    "\n",
    "## Disadvantages of K-NN in images\n",
    "- very slow at test time\n",
    "- distance metrics on pixels are not informative\n",
    "    - some boxed, shifted, tinted and normal images might have similar L2 distance but the information in the image might be hugely different\n",
    "- Curse of dimensionality (requires number of examples on the exponential scale w.r.t. number of dimensions)\n",
    "- K-NN on images is almost never used\n",
    "\n",
    "## Linear Classification\n",
    "- this will generalize quite well to large neural networks\n",
    "\n",
    "### Dataset\n",
    "- CIFAR10\n",
    "- 50000 training dataset over 10 classes\n",
    "- 10000 test images\n",
    "-32x32x3\n",
    "\n",
    "### Parametric approach\n",
    "- function approximation that takes an image and outputs n numbers as probability weight for each class\n",
    "-summraize the knowledge of the training data into parameters and then use these optimized parameters for inference\n",
    "- bias - data independence bias term for our data (larger for the class that has larger number of examples if we have unbalanced data), independence balance offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes from Lecture 3\n",
    "\n",
    "- Linear classifier is one of the major building blocks of neural networks\n",
    "- define a **loss function** that quantifies our unhappiness with the scores accross the training data\n",
    "- come up with a way of **efficiently finding the parameters that minimize the loss function (optimization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
