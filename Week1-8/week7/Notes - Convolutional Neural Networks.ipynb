{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared By:\n",
    "- Ashish Sharma <accssharma@gmail.com>\n",
    "- AI Saturdays - Week 7 (July)\n",
    "- AI Developers, Boise \n",
    "\n",
    "# Resources:\n",
    "- [Live CNN training demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)\n",
    "- [k-nearest neighbor demo](vision.stanford.edu/teaching/cs231n-demos/knn/)\n",
    "- [Tensorflow - train your first neural network](https://www.tensorflow.org/tutorials/keras/basic_classification)\n",
    "\n",
    "# Notes - Convolutional Neural Networks\n",
    "\n",
    "- cs231n 2016 winter [lecture 5](https://www.youtube.com/watch?v=gYpoJMlgyXA) and [lecture 6](https://www.youtube.com/watch?v=hd_KFJ5ktUc)\n",
    "\n",
    "## Myth\n",
    "- ConvNets need a lot of data to train\n",
    "- Well if you have a small dataset, we rarely ever train ConvNets from scratch. We finetune the pre-trained models. Transfer learning.\n",
    "- Download such pre-trained networks from caffe-zoo kind of models repo.\n",
    "\n",
    "## Why even use any non-linear function?\n",
    "- If not used, then your neural network is a whole total linear sandwich of NN layers.\n",
    "- Non-linear functions give all these wiggles to help you train all these networks\n",
    "\n",
    "\n",
    "## Training a neural network is as simple as four step (Mini-batch SGD):\n",
    "- Sample a batch of data\n",
    "- Forward prop through the graph, get loss\n",
    "- Backprop to calculate the gradients\n",
    "- Update the parameters using the gradient\n",
    "\n",
    "\n",
    "## Training a Neural Network\n",
    "- 1957 - Frank Rosenblatt came up with the idea of Perceptron [[Original Paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)]\n",
    "    - Three main questions:\n",
    "        - How is information about the physical world sensed or detected? (province of sensory physiology)\n",
    "        - How is that information stored or remembered?\n",
    "        - How does that information in storage or memory influence recognition and behavior?\n",
    "    - \n",
    "    \n",
    "- non-linear - step function, which is not a differential function\n",
    "- no backprop, no loss function concept\n",
    "- 1960- Widrow and Hoff - circuit that can learn\n",
    "- 1986 - First time back-propagation became popular (Rumelhart et al)\n",
    "    - could not scale well\n",
    "    - stayed same for almost 20 years\n",
    "- 2006 - Hinton and Salakhutdinov\n",
    "    - Reinvigorated research in deep learning\n",
    "    - RBM\n",
    "    - Backprop works, but you have to be careful in initialization\n",
    "- 2010 - GMM/HMM concept - Microsoft in Speech Recognition\n",
    "- 2012 - Convnet - AlexNet\n",
    "\n",
    "- Why did they start working?\n",
    "    - Parameter initialization\n",
    "    - GPUs\n",
    "    - More Data\n",
    "    \n",
    "- Overview of Training Neural Networks\n",
    "    - One time setup\n",
    "        - activation function\n",
    "        - preprocessing\n",
    "        - weight initialization\n",
    "        - regularization\n",
    "        - gradient checking\n",
    "    - Training Dynamics\n",
    "        - babysitting the learning process\n",
    "        - parameter updates\n",
    "        - hyperparameter optimization\n",
    "    - Evaluation\n",
    "        - model ensembles\n",
    "        \n",
    "        \n",
    "### Activation Functions\n",
    "- Sigmoid [Historically commonly used]\n",
    "    - Saturated neurons (very closed to 0 and 1 \"kill\" the gradients)\n",
    "    - As Backprop works with chain rule of derivative, but since the sigmoid values near to 0 and 1 would have very small (~0) derivative, so when they are passed along to the earlier layers in the neurons, then the gradients stop flowing to the earlier layers.\n",
    "    - Sigmoid values near 0 and 1 are in saturated regime\n",
    "    - Sigmoid works well only with the sigmoid outputs in the active regimes, values not close to 0 and 1 but close to 0.5.\n",
    "    - Sigmoid outputs are not zero-centric\n",
    "    - exp() is a bit compute expensive part of the non linearity\n",
    "\n",
    "- tanh\n",
    "    - LeCun 1991 recommended tanh instead of sigmoid\n",
    "    - squashes numbers to range [-1,1]\n",
    "    - zero centered\n",
    "    - still kills gradient when saturated\n",
    "    \n",
    "- ReLU\n",
    "    - max(0,x) - converges much faster than sigmoid/tanh in practice (6X)\n",
    "    - does not saturate (in +region)\n",
    "    - very computationally efficient\n",
    "    - Not zero-centric output\n",
    "    - An annoyance\n",
    "        - what is the gradient when x<0\n",
    "    - Be wary of:\n",
    "        - dead ReLU\n",
    "        - people like to initialize ReLU neurons with slightly positive biases (eg. 0.001)\n",
    "\n",
    "- Leaky ReLU\n",
    "    - f(x) = max(0.01x, x)\n",
    "        - don't let ReLU die\n",
    "    - does not saturate\n",
    "    - computationally efficient\n",
    "    - converges much faster (6X)\n",
    "\n",
    "\n",
    "- Parametric Rectifier\n",
    "    - max(alpha* x, x) - backprop into alpha (parameter)\n",
    "    \n",
    "- Exponential Linear Units (ELU)\n",
    "    - all benefits of ReLU\n",
    "    - Does not die\n",
    "    - Closer to zero mean outputs\n",
    "    - comutation requires exp() - downside\n",
    "    \n",
    "- Maxout \"Neuron\"\n",
    "    - Generalizes ReLU and Leaky ReLU\n",
    "    - Linear Regime, does not saturate, does not die!\n",
    "    - does not have the basic form of dot product -> non linearity\n",
    "    - doubles the number of parameters/neuron - downside\n",
    "    \n",
    "- TLDR\n",
    "    - use ReLU, be careful with your learning rates\n",
    "    - tryo out ReLU/MaxOut/ELU\n",
    "    - Try out tanh but don't expect much\n",
    "    - Don't use sigmoid\n",
    "    \n",
    "    \n",
    "### Data Preprocessing\n",
    "- zero-centered data, subtract by mean\n",
    "- normalized data (normalize by st dev, normalization) - in images that't not as common\n",
    "- see PCA and whitening of the data\n",
    "\n",
    "### Images \n",
    "- Subtract the mean image (eg. Alexnet) (mean image = [32,32,3] array)\n",
    "- Subtract per-channel mean (man along each channel = 3 number)\n",
    "- not common to normalize variance, to do PCA and whitening in Images\n",
    "\n",
    "### Weight initialization\n",
    "- one of the reasons that the early NNs did not work\n",
    "- How not to do weight initialization?\n",
    "    - W = 0\n",
    "        - no symmetry breaking\n",
    "        - all neurons compute the same thing\n",
    "    - small random numbers\n",
    "        - gaussian with zero mean and 1e-2 std\n",
    "        - W = 0.01* np.random(D,H)\n",
    "        - works ~0kay for small networks, but can lead to non-homogeneous distributions of activations across the layers of a network\n",
    "        \n",
    "     - Xavier initialization\n",
    "         - works reasonably well only with linearity\n",
    "         - does not work with non-linearity\n",
    "         - but when using the ReLU nonlinearity, it breaks\n",
    "         - He et al., additional /2\n",
    "     - you always want roughly unit gaussian activations!\n",
    "     - SOLUTION: Batch Normalization\n",
    "\n",
    "### Batch Normalization:\n",
    "    - you want unit gaussian activations? just make them so?\n",
    "    - perfectly differentiable function\n",
    "    - new layers - perfectly differentiable\n",
    "    - X minibatch, D neurons\n",
    "    - Approach: \n",
    "        - compute the empirical mean and variance independently for each dimension\n",
    "        - Normalize with variance accross the minibatch\n",
    "        - scale and shift\n",
    "    - Usually inserted after FC and ConvNet before activation/non-linearity layer\n",
    "    - Pros\n",
    "        - improves gradient flow through the network\n",
    "        - allows higher learning rate\n",
    "        - [V IMP] reduce the strong dependence on initialization\n",
    "        - acts as a form of regularization in a funny way, and slightly reduces the need for dropout, may be!\n",
    "            - stochastically jittering\n",
    "    - At test time, BatchNorm layer functions differently\n",
    "        - The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used\n",
    "        - eg. can be estimated during training with running averages\n",
    "    - Cons\n",
    "        - slow down penalty\n",
    "        - someone pointed may be 30% overhead\n",
    "        \n",
    "    \n",
    "### Choose the architecture\n",
    "- say we start one hidden layer of 50 neurons\n",
    "    - weights and biases initialization\n",
    "- disable regularization\n",
    "- returns the loss and the gradient of the parameters\n",
    "- crank up regularization, loss went \n",
    "- Hyper parameter optimization\n",
    "- couarse to fine cross-validation stages\n",
    "- learning rates: sample and optimize in log space\n",
    "- grid search of hyperparameters\n",
    "    - Grid layout vs Random layout\n",
    "    - Always use random layout\n",
    "- nw architecture\n",
    "- monitor and visualize loss curve\n",
    "- Training vs validation accuracy\n",
    "    - big gap - overfitting\n",
    "    - no gap - increase model capacity\n",
    "- Track the ratio of weight updates/weight magnitudes\n",
    "\n",
    "\n",
    "- Takeaways\n",
    "    - we do not want our outputs to be in killed region which results in zero or near zero gradients\n",
    "    - \n",
    "    \n",
    "## TODO\n",
    "- parameter update schemes\n",
    "- learning rate schedules\n",
    "- Dropout\n",
    "- Gradient Checking\n",
    "- MOdel ensembles\n",
    "\n",
    "### Parameter Updates\n",
    "- Stochastic Gradient Descent\n",
    "    - What is the trajectory along which we converge towards the minimum with SGD?\n",
    "        - bounce alot\n",
    "    - Solution: Momentum update and integrate velocity\n",
    "    - allows building up velocity\n",
    "- Momentum update\n",
    "    - SGD has a problem where updates are faster in steep direction and bounce back and forth in the shallow direction\n",
    "    - physical interpretation as ball rolling down the loss function + friction (mu coefficient)\n",
    "    - velocity decays to zero update with friction concept\n",
    "   \n",
    "- Nesterov Momentum update\n",
    "- AdaGrad update\n",
    "    - per parameter adaptive gradient\n",
    "    - shallow gradients are scaled up\n",
    "    - deep gradients are decayed\n",
    "    \n",
    "- RMSProp Update\n",
    "    - vanilla implementation of AdaGrad would decay to zero UPDATES\n",
    "    - but Geoff Hinton's approach with decay rate helps avoiding the final decay to zero UPDATES\n",
    "    \n",
    "- Adam\n",
    "    - AdaGrad + Momentum\n",
    " \n",
    "- All update approaches use LR as hyperparaemeter\n",
    "    - decay learning rate eg. step decay: by half every few epochs )\n",
    "    - exponential decay\n",
    "    - 1/t decay\n",
    "    \n",
    "## Optimization Techniques\n",
    "\n",
    "### Second order optimization methods\n",
    "- faster\n",
    "- no hyperparameter\n",
    "- second order taylor expansion\n",
    "- eg. Newton parameter udpate\n",
    "- Why is it impractical in Neural Networks?\n",
    "    - involves Hessian matrix\n",
    "    - imagine Hessian matrix of 100000 parameters and inverting it\n",
    "\n",
    "### L-BFGS\n",
    "- usually works very well in full batch\n",
    "- gives bad results when tried to transfer to mini-batch setup - active research\n",
    "\n",
    "## Suggestion\n",
    "- Use Adam \n",
    "- Use L-BFGS if you can afford to do full batch updates\n",
    "\n",
    "## Ensembles\n",
    "\n",
    "## Regularization\n",
    "\n",
    "### Dropout\n",
    "- p : probability of choosing a mask of dropping out neurons\n",
    "- randomly drop the unit - activation\n",
    "- effects both forward and backward direction (weights associated witht he dropped units aren't updated)\n",
    "    - SEE HOW DOES IT EFFECT GRADIENT UPDATES OF THE DROPPED NEURONS?\n",
    "    - Back propagate the mask as well\n",
    "- How could this possibly be a good idea?\n",
    "    - prevent overfitting?\n",
    "    - control of your variance of your model \n",
    "    - see bias/variance trade off\n",
    "    - mny ensembles of smaller neural network - randomly\n",
    "    - allows function approximation more effectively and not let specific featuers to highly affect the output space (eg. cat classification)\n",
    "    - training a large ensemble of models (that share parameters)\n",
    "        - your sub-sampling your NN\n",
    "    - Each binary mask is one model, gets trained on only ~one data point\n",
    "- At test time\n",
    "    - generally, no dropout on test image in forward pass\n",
    "    - scale the activations on the dropped out layers by p\n",
    "    - we must scale the activations so that for each neuron: output at test time = expected output at training time\n",
    "- MORE COMMON: Inverted dropout\n",
    "\n",
    "### Gradient Checking\n",
    "- see class notes\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
